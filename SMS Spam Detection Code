#SMS Spam Detection using Natural Language Processing with Python
#Required Libraries

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

nltk.download('wordnet')
nltk.download('stopwords')

#1.Data Gathering
df = pd.read_csv("/content/sample_data/SMSSpamCollection.txt", sep = '\t', names = ['Label','Msg'] )
df.head()

#2.Exploratory Data Analysis
df.info()
df.isna().sum()
df['Label'].value_counts()

#3.Data Preprocessing
corpus = []
lm = WordNetLemmatizer()
for i in range (len(df)):
    review = re.sub('^a-zA-Z0-9',' ',df['Msg'][i])
    review = review.lower()
    review = review.split()
    review = [data for data in review if data not in stopwords.words('english')]
    review = [lm.lemmatize(data) for data in review]
    review = " ".join(review)
    corpus.append(review) 

df['Msg'][0]

len(df['Msg'])

len(corpus)

df['Msg']=corpus
df.head()

#4.Model Building
#4.1 Data Splitting
x = df['Msg']
y = df['Label']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state = 10)

len(x_train), len(y_train)

len(x_test),len(y_test)

#4.2 Vectorization (Convert Text Data Into The Vectors)
tf_obj = TfidfVectorizer()
x_train_tfidf = tf_obj.fit_transform(x_train).toarray()
x_train_tfidf

x_train_tfidf.shape

text_mnb = Pipeline([('tfidf',TfidfVectorizer()),('mnb',MultinomialNB())])

text_mnb.fit(x_train,y_train)

Pipeline(steps=[('tfidf', TfidfVectorizer()), ('mnb', MultinomialNB())])

#Accuracy Score on Testing Data
y_pred_test = text_mnb.predict(x_test)
print("Accuracy Score:", accuracy_score(y_test,y_pred_test)*100)

#Accuracy Score on Training Data
y_pred_train = text_mnb.predict(x_train)
print("Accuracy Score:",accuracy_score(y_train,y_pred_train)*100)

#Confusion Matrix on Testing Data
y_pred_test = text_mnb.predict(x_test)
print("Confusion Matrix on Test Data:\n", confusion_matrix(y_test,y_pred_test))

#Classification Report on Testing Data
y_pred_test = text_mnb.predict(x_test)
print("Classification Reportx on Test Data:\n", classification_report(y_test,y_pred_test))

def preprocess_data(text):
    review = re.sub('^a-zA-Z0-9',' ',text)
    review = review.lower()
    review = review.split()
    review = [data for data in review if data not in stopwords.words('english')]
    review = [lm.lemmatize(data) for data in review]
    review = " ".join(review)
    return [review]


user_data = df['Msg'][0]
print(user_data)
user_data = preprocess_data(user_data)
user_data

text_mnb.predict(user_data)[0]

class prediction:
    
    def __init__(self,data):
        self.data = data
        
    def user_data_preprocessing(self):
        lm = WordNetLemmatizer()
        review = re.sub('^a-zA-Z0-9',' ',self.data)
        review = review.lower()
        review = review.split()
        review = [data for data in review if data not in stopwords.words('english')]
        review = [lm.lemmatize(data) for data in review]
        review = " ".join(review)
        return [review]
    
    def user_data_prediction(self):
        preprocess_data = self.user_data_preprocessing()
        
        if text_mnb.predict(preprocess_data)[0] == 'spam':
            return 'This Message is Spam'
            
        else:
            return 'This Message is Ham'  

df.head()

user_data = df['Msg'][2]
print(user_data)
prediction(user_data).user_data_prediction()

user_data = df['Msg'][3]
print(user_data)
prediction(user_data).user_data_prediction()
